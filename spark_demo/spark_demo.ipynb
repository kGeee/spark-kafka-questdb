{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook based on https://towardsdatascience.com/building-a-real-time-prediction-pipeline-using-spark-structured-streaming-and-microservices-626dc20899eb\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import random\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka producer variables\n",
    "\n",
    "simple_messages = [\n",
    "'I love this pony',\n",
    "'This restaurant is great',\n",
    "'The weather is bad today',\n",
    "'I will go to the beach this weekend',\n",
    "'She likes to swim',\n",
    "'Apple is a great company'\n",
    "]\n",
    "\n",
    "bootstrap_servers='kafka:9092'\n",
    "topic='test'\n",
    "msg_count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):\n",
    "    \"\"\" Called once for each message produced to indicate delivery result.\n",
    "        Triggered by poll() or flush(). \"\"\"\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {}'.format(msg.topic()))\n",
    "\n",
    "def confluent_kafka_producer():\n",
    "\n",
    "    p = Producer({'bootstrap.servers': bootstrap_servers})\n",
    "    for data in simple_messages:\n",
    "        \n",
    "        record_key = str(uuid.uuid4())\n",
    "        record_value = json.dumps({'data': data})\n",
    "        \n",
    "        p.produce(topic, key=record_key, value=record_value, on_delivery=delivery_report)\n",
    "        p.poll(0)\n",
    "\n",
    "    p.flush()\n",
    "    print('we\\'ve sent {count} messages to {brokers}'.format(count=len(simple_messages), brokers=bootstrap_servers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "Message delivered to test\n",
      "we've sent 6 messages to kafka:9092\n"
     ]
    }
   ],
   "source": [
    "confluent_kafka_producer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('RealtimeKafkaML') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_raw = spark \\\n",
    "  .readStream \\\n",
    "  .format('kafka') \\\n",
    "  .option('kafka.bootstrap.servers', \"broker:29092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option('subscribe', \"alt_liquidations\") \\\n",
    "  .load()\n",
    "\n",
    "df_json = df_raw.selectExpr('CAST(value AS STRING) as json')\n",
    "\n",
    "schema = StructType([StructField('ticker', StringType()),\\\n",
    "                            StructField('amount', FloatType()),\\\n",
    "                            StructField('side', StringType()),\\\n",
    "                            StructField('price', FloatType()),\n",
    "                             StructField('timestamp', StringType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_json.select(from_json(df_json.json, schema).alias('data')) \\\n",
    "        .groupBy(\"data.ticker\") \\\n",
    "        .agg(sum(\"data.amount\").alias(\"total_liq\")) \\\n",
    "        .sort(desc(\"total_liq\")) \\\n",
    "        .writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a small batch of data from kafka and display to the console\n",
    "\n",
    "schema = StructType([StructField('data', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema).alias('raw_data')) \\\n",
    "  .select('raw_data.data') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test service\n",
    "import requests\n",
    "import json\n",
    "\n",
    "data_jsons = '{\"data\":\"' + simple_messages[1] + '\"}'\n",
    "print(data_jsons)\n",
    "result = requests.post('http://127.0.0.1:9000/predict', json=json.loads(data_jsons))\n",
    "print(json.dumps(result.json()))\n",
    "\n",
    "#vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_analysis(data):\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    result = requests.post('http://localhost:9000/predict', json=json.loads(data))\n",
    "    return json.dumps(result.json())\n",
    "\n",
    "def apply_sum(data):\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    result = requests.post('http://localhost:9000/sum', json=json.loads(data))\n",
    "    return json.dumps(result.json())\n",
    "\n",
    "vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())\n",
    "sum_udf = udf(lambda data: apply_sum(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_239/344976144.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                             StructField('compound', StringType())])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mschema_sum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m df_json.select(from_json(df_json.json, schema_input).alias('sentence'),\\\n\u001b[0m\u001b[1;32m     10\u001b[0m                from_json(vader_udf(df_json.json), schema_predict_output).alias('response'))\\\n\u001b[1;32m     11\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentence.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'response.*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(col, schema, options)\u001b[0m\n\u001b[1;32m   8298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8299\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8300\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8301\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8302\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_options_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     \u001b[0mInvokes\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "schema_input = StructType([StructField('data', StringType())])\n",
    "schema_predict_output = StructType([StructField('neg', StringType()),\\\n",
    "                            StructField('pos', StringType()),\\\n",
    "                            StructField('neu', StringType()),\\\n",
    "                            StructField('compound', StringType())])\n",
    "\n",
    "schema_sum_output = StructType([StructField('count', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema_input).alias('sentence'),\\\n",
    "               from_json(vader_udf(df_json.json), schema_predict_output).alias('response'))\\\n",
    "  .select('sentence.data', 'response.*') \\\n",
    "  .distinct() \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_sink():\n",
    "    crypto_sink = {\n",
    "        \"name\": \"questdb-sink-crypto\",\n",
    "            \"config\": {\n",
    "                \"connector.class\":\"io.questdb.kafka.QuestDBSinkConnector\",\n",
    "                \"tasks.max\":\"1\",\n",
    "                \"topics\": \"topic_ETH, topic_BTC, topic_SOL\",\n",
    "                \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n",
    "                \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "                \"key.converter.schemas.enable\": \"false\",\n",
    "                \"value.converter.schemas.enable\": \"false\",\n",
    "                \"host\": \"questdb\",\n",
    "                \"timestamp.field.name\": \"timestamp\",\n",
    "                \"symbols\":\"currency\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    result = requests.post('http://127.0.0.1:8083/connectors', json=crypto_sink)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptofeed import FeedHandler\n",
    "from cryptofeed.backends.kafka import BookKafka, TradeKafka\n",
    "from cryptofeed.defines import L2_BOOK, TRADES, L3_BOOK\n",
    "from cryptofeed.exchanges import Coinbase, Binance, Bitfinex\n",
    "\n",
    "\n",
    "def produuucer():\n",
    "    f = FeedHandler()\n",
    "    HOSTNAME = 'broker:29092'\n",
    "    cbs = {TRADES: TradeKafka(bootstrap_servers=HOSTNAME), L2_BOOK: BookKafka(bootstrap_servers=HOSTNAME)}\n",
    "\n",
    "    # Add trade and lv 2 bitcoin data to Feed\n",
    "    f.add_feed(Bitfinex(max_depth=25, channels=[TRADES, L2_BOOK], symbols=['BTC-USD'], callbacks=cbs))\n",
    "    # f.add_feed(Binance(max_depth=25, channels=[TRADES, L2_BOOK], symbols=['BTC-BUSD'], callbacks=cbs))\n",
    "    \n",
    "    # Example of how to extract level 3 order book data\n",
    "    # f.add_feed(Coinbase(max_depth=25, channels=[TRADES, L2_BOOK], symbols=['BTC-USD'], callbacks=cbs))\n",
    "\n",
    "    f.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 00:31:29,415 : ERROR : FH: Unhandled RuntimeError('This event loop is already running') - shutting down\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/cryptofeed/feedhandler.py\", line 151, in run\n",
      "    loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._check_running()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 588, in _check_running\n",
      "    raise RuntimeError('This event loop is already running')\n",
      "RuntimeError: This event loop is already running\n",
      "2023-07-14 00:31:29,415 : ERROR : FH: Unhandled RuntimeError('This event loop is already running') - shutting down\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/cryptofeed/feedhandler.py\", line 151, in run\n",
      "    loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._check_running()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/base_events.py\", line 588, in _check_running\n",
      "    raise RuntimeError('This event loop is already running')\n",
      "RuntimeError: This event loop is already running\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m produuucer()\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mproduuucer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m f\u001b[39m.\u001b[39madd_feed(Bitfinex(max_depth\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m, channels\u001b[39m=\u001b[39m[TRADES, L2_BOOK], symbols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mBTC-USD\u001b[39m\u001b[39m'\u001b[39m], callbacks\u001b[39m=\u001b[39mcbs))\n\u001b[1;32m     14\u001b[0m \u001b[39m# f.add_feed(Binance(max_depth=25, channels=[TRADES, L2_BOOK], symbols=['BTC-BUSD'], callbacks=cbs))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m# Example of how to extract level 3 order book data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# f.add_feed(Coinbase(max_depth=25, channels=[TRADES, L2_BOOK], symbols=['BTC-USD'], callbacks=cbs))\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m f\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/cryptofeed/feedhandler.py:157\u001b[0m, in \u001b[0;36mFeedHandler.run\u001b[0;34m(self, start_loop, install_signal_handlers, exception_handler)\u001b[0m\n\u001b[1;32m    155\u001b[0m     LOG\u001b[39m.\u001b[39mexception(\u001b[39m'\u001b[39m\u001b[39mFH: Unhandled \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m - shutting down\u001b[39m\u001b[39m'\u001b[39m, why)\n\u001b[1;32m    156\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstop(loop\u001b[39m=\u001b[39;49mloop)\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose(loop\u001b[39m=\u001b[39mloop)\n\u001b[1;32m    160\u001b[0m LOG\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mFH: leaving run()\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/cryptofeed/feedhandler.py:195\u001b[0m, in \u001b[0;36mFeedHandler.stop\u001b[0;34m(self, loop)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstop\u001b[39m(\u001b[39mself\u001b[39m, loop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    194\u001b[0m     shutdown_tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop(loop\u001b[39m=\u001b[39mloop)\n\u001b[0;32m--> 195\u001b[0m     loop\u001b[39m.\u001b[39;49mrun_until_complete(asyncio\u001b[39m.\u001b[39;49mgather(\u001b[39m*\u001b[39;49mshutdown_tasks))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:629\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[39mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m--> 629\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[1;32m    631\u001b[0m new_task \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m futures\u001b[39m.\u001b[39misfuture(future)\n\u001b[1;32m    632\u001b[0m future \u001b[39m=\u001b[39m tasks\u001b[39m.\u001b[39mensure_future(future, loop\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/base_events.py:588\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[0;32m--> 588\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    591\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "produuucer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
